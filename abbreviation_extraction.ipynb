{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import element\n",
    "from abbreviations import schwartz_hearst\n",
    "import subprocess\n",
    "import json\n",
    "import logging\n",
    "import regex\n",
    "import sys\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schwartz Hearst method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Python 3 refactoring of Vincent Van Asch's Python 2 code at\n",
    "\n",
    "http://www.cnts.ua.ac.be/~vincent/scripts/abbreviations.py\n",
    "\n",
    "Based on\n",
    "\n",
    "A Simple Algorithm for Identifying Abbreviations Definitions in Biomedical Text\n",
    "A. Schwartz and M. Hearst\n",
    "Biocomputing, 2003, pp 451-462.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Candidate(str):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.start = 0\n",
    "        self.stop = 0\n",
    "\n",
    "    def set_position(self, start, stop):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "\n",
    "\n",
    "def yield_lines_from_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = line.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                line = line.decode('latin-1').encode('utf-8').decode('utf-8')\n",
    "            line = line.strip()\n",
    "            yield line\n",
    "\n",
    "\n",
    "def yield_lines_from_doc(doc_text):\n",
    "    for line in doc_text.split(\".\"):\n",
    "        yield line.strip()\n",
    "\n",
    "\n",
    "def best_candidates(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence: line read from input file\n",
    "    :return: a Candidate iterator\n",
    "    \"\"\"\n",
    "\n",
    "    if '(' in sentence:\n",
    "        # Check some things first\n",
    "        if sentence.count('(') != sentence.count(')'):\n",
    "            raise ValueError(\"Unbalanced parentheses: {}\".format(sentence))\n",
    "\n",
    "        if sentence.find('(') > sentence.find(')'):\n",
    "            raise ValueError(\"First parentheses is right: {}\".format(sentence))\n",
    "\n",
    "        close_index = -1\n",
    "        while 1:\n",
    "            # Look for open parenthesis. Need leading whitespace to avoid matching mathematical and chemical formulae\n",
    "            open_index = sentence.find(' (', close_index + 1)\n",
    "\n",
    "            if open_index == -1: break\n",
    "\n",
    "            # Advance beyond whitespace\n",
    "            open_index += 1\n",
    "\n",
    "            # Look for closing parentheses\n",
    "            close_index = open_index + 1\n",
    "            open_count = 1\n",
    "            skip = False\n",
    "            while open_count:\n",
    "                try:\n",
    "                    char = sentence[close_index]\n",
    "                except IndexError:\n",
    "                    # We found an opening bracket but no associated closing bracket\n",
    "                    # Skip the opening bracket\n",
    "                    skip = True\n",
    "                    break\n",
    "                if char == '(':\n",
    "                    open_count += 1\n",
    "                elif char in [')', ';', ':']:\n",
    "                    open_count -= 1\n",
    "                close_index += 1\n",
    "\n",
    "            if skip:\n",
    "                close_index = open_index + 1\n",
    "                continue\n",
    "\n",
    "            # Output if conditions are met\n",
    "            start = open_index + 1\n",
    "            stop = close_index - 1\n",
    "            candidate = sentence[start:stop]\n",
    "\n",
    "            # Take into account whitespace that should be removed\n",
    "            start = start + len(candidate) - len(candidate.lstrip())\n",
    "            stop = stop - len(candidate) + len(candidate.rstrip())\n",
    "            candidate = sentence[start:stop]\n",
    "            #print (candidate)\n",
    "\n",
    "            if conditions(candidate):\n",
    "                new_candidate = Candidate(candidate)\n",
    "                new_candidate.set_position(start, stop)\n",
    "                yield new_candidate\n",
    "            #elif LF_in_parentheses:\n",
    "\n",
    "\n",
    "def conditions(candidate):\n",
    "    \"\"\"\n",
    "    Based on Schwartz&Hearst\n",
    "\n",
    "    2 <= len(str) <= 10\n",
    "    len(tokens) <= 2\n",
    "    re.search(r'\\p{L}', str)\n",
    "    str[0].isalnum()\n",
    "\n",
    "    and extra:\n",
    "    if it matches (\\p{L}\\.?\\s?){2,}\n",
    "    it is a good candidate.\n",
    "\n",
    "    :param candidate: candidate abbreviation\n",
    "    :return: True if this is a good candidate\n",
    "    \"\"\"\n",
    "    LF_in_parentheses=False\n",
    "    viable = True\n",
    "    if regex.match(r'(\\p{L}\\.?\\s?){2,}', candidate.lstrip()):\n",
    "        viable = True\n",
    "    if len(candidate) < 2 or len(candidate) > 10:\n",
    "        viable = False\n",
    "    if len(candidate.split()) > 2:\n",
    "        viable = False\n",
    "        LF_in_parentheses=True                #customize funcition find LF in parentheses\n",
    "    if candidate.islower():                   #customize funcition discard all lower case candidate\n",
    "        viable = False\n",
    "    if not regex.search(r'\\p{L}', candidate): # \\p{L} = All Unicode letter\n",
    "        viable = False\n",
    "    if not candidate[0].isalnum():\n",
    "        viable = False\n",
    "\n",
    "    return viable\n",
    "\n",
    "\n",
    "def get_definition(candidate, sentence):\n",
    "    \"\"\"\n",
    "    Takes a candidate and a sentence and returns the definition candidate.\n",
    "\n",
    "    The definition candidate is the set of tokens (in front of the candidate)\n",
    "    that starts with a token starting with the first character of the candidate\n",
    "\n",
    "    :param candidate: candidate abbreviation\n",
    "    :param sentence: current sentence (single line from input file)\n",
    "    :return: candidate definition for this abbreviation\n",
    "    \"\"\"\n",
    "    # Take the tokens in front of the candidate\n",
    "    tokens = regex.split(r'[\\s\\-]+', sentence[:candidate.start - 2].lower())\n",
    "    # the char that we are looking for\n",
    "    key = candidate[0].lower()\n",
    "\n",
    "    # Count the number of tokens that start with the same character as the candidate\n",
    "    first_chars = [t[0] for t in filter(None, tokens)]\n",
    "\n",
    "    definition_freq = first_chars.count(key)\n",
    "    candidate_freq = candidate.lower().count(key)\n",
    "\n",
    "    # Look for the list of tokens in front of candidate that\n",
    "    # have a sufficient number of tokens starting with key\n",
    "    if candidate_freq <= definition_freq:\n",
    "        # we should at least have a good number of starts\n",
    "        count = 0\n",
    "        start = 0\n",
    "        start_index = len(first_chars) - 1\n",
    "        while count < candidate_freq:\n",
    "            if abs(start) > len(first_chars):\n",
    "                raise ValueError(\"candidate {} not found\".format(candidate))\n",
    "            start -= 1\n",
    "            # Look up key in the definition\n",
    "            try:\n",
    "                start_index = first_chars.index(key, len(first_chars) + start)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            # Count the number of keys in definition\n",
    "            count = first_chars[start_index:].count(key)\n",
    "\n",
    "        # We found enough keys in the definition so return the definition as a definition candidate\n",
    "        start = len(' '.join(tokens[:start_index]))\n",
    "        stop = candidate.start - 1\n",
    "        candidate = sentence[start:stop]\n",
    "\n",
    "        # Remove whitespace\n",
    "        start = start + len(candidate) - len(candidate.lstrip())\n",
    "        stop = stop - len(candidate) + len(candidate.rstrip())\n",
    "        candidate = sentence[start:stop]\n",
    "\n",
    "        new_candidate = Candidate(candidate)\n",
    "        new_candidate.set_position(start, stop)\n",
    "        return new_candidate\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There are less keys in the tokens in front of candidate than there are in the candidate')\n",
    "\n",
    "\n",
    "def select_definition(definition, abbrev):\n",
    "    \"\"\"\n",
    "    Takes a definition candidate and an abbreviation candidate\n",
    "    and returns True if the chars in the abbreviation occur in the definition\n",
    "\n",
    "    Based on\n",
    "    A simple algorithm for identifying abbreviation definitions in biomedical texts, Schwartz & Hearst\n",
    "    :param definition: candidate definition\n",
    "    :param abbrev: candidate abbreviation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(definition) < len(abbrev):\n",
    "        raise ValueError('Abbreviation is longer than definition')\n",
    "\n",
    "    if abbrev in definition.split():\n",
    "        raise ValueError('Abbreviation is full word of definition')\n",
    "\n",
    "    s_index = -1\n",
    "    l_index = -1\n",
    "\n",
    "    while 1:\n",
    "        try:\n",
    "            long_char = definition[l_index].lower()\n",
    "        except IndexError:\n",
    "            raise\n",
    "\n",
    "        short_char = abbrev[s_index].lower()\n",
    "\n",
    "        if not short_char.isalnum():\n",
    "            s_index -= 1\n",
    "\n",
    "        if s_index == -1 * len(abbrev):\n",
    "            if short_char == long_char:\n",
    "                if l_index == -1 * len(definition) or not definition[l_index - 1].isalnum():\n",
    "                    break\n",
    "                else:\n",
    "                    l_index -= 1\n",
    "            else:\n",
    "                l_index -= 1\n",
    "                if l_index == -1 * (len(definition) + 1):\n",
    "                    raise ValueError(\"definition {} was not found in {}\".format(abbrev, definition))\n",
    "\n",
    "        else:\n",
    "            if short_char == long_char:\n",
    "                s_index -= 1\n",
    "                l_index -= 1\n",
    "            else:\n",
    "                l_index -= 1\n",
    "\n",
    "    new_candidate = Candidate(definition[l_index:len(definition)])\n",
    "    new_candidate.set_position(definition.start, definition.stop)\n",
    "    definition = new_candidate\n",
    "\n",
    "    tokens = len(definition.split())\n",
    "    length = len(abbrev)\n",
    "\n",
    "    if tokens > min([length + 5, length * 2]):\n",
    "        raise ValueError(\"did not meet min(|A|+5, |A|*2) constraint\")\n",
    "\n",
    "    # Do not return definitions that contain unbalanced parentheses\n",
    "    if definition.count('(') != definition.count(')'):\n",
    "        raise ValueError(\"Unbalanced parentheses not allowed in a definition\")\n",
    "\n",
    "    return definition\n",
    "\n",
    "\n",
    "def extract_abbreviation_definition_pairs(file_path=None,\n",
    "                                          doc_text=None,\n",
    "                                          most_common_definition=False,\n",
    "                                          first_definition=False,\n",
    "                                          all_definition=False):\n",
    "    abbrev_map = dict()\n",
    "    list_abbrev_map = defaultdict(list)\n",
    "    counter_abbrev_map = dict()\n",
    "    omit = 0\n",
    "    written = 0\n",
    "    if file_path:\n",
    "        sentence_iterator = enumerate(yield_lines_from_file(file_path))\n",
    "    elif doc_text:\n",
    "        sentence_iterator = enumerate(yield_lines_from_doc(doc_text))\n",
    "    else:\n",
    "        return abbrev_map\n",
    "\n",
    "    collect_definitions = False\n",
    "    if most_common_definition or first_definition or all_definition:\n",
    "        collect_definitions = True\n",
    "\n",
    "    for i, sentence in sentence_iterator:\n",
    "        # Remove any quotes around potential candidate terms\n",
    "        clean_sentence = regex.sub(r'([(])[\\'\"\\p{Pi}]|[\\'\"\\p{Pf}]([);:])', r'\\1\\2', sentence)\n",
    "        try:\n",
    "            for candidate in best_candidates(clean_sentence):\n",
    "                try:\n",
    "                    definition = get_definition(candidate, clean_sentence)\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    log.debug(\"{} Omitting candidate {}. Reason: {}\".format(i, candidate, e.args[0]))\n",
    "                    omit += 1\n",
    "                else:\n",
    "                    try:\n",
    "                        definition = select_definition(definition, candidate)\n",
    "                    except (ValueError, IndexError) as e:\n",
    "                        log.debug(\"{} Omitting definition {} for candidate {}. Reason: {}\".format(i, definition, candidate, e.args[0]))\n",
    "                        omit += 1\n",
    "                    else:\n",
    "                        # Either append the current definition to the list of previous definitions ...\n",
    "                        if collect_definitions:\n",
    "                            list_abbrev_map[candidate].append(definition)\n",
    "                        else:\n",
    "                            # Or update the abbreviations map with the current definition\n",
    "                            abbrev_map[candidate] = definition\n",
    "                        written += 1\n",
    "        except (ValueError, IndexError) as e:\n",
    "            log.debug(\"{} Error processing sentence {}: {}\".format(i, sentence, e.args[0]))\n",
    "    log.debug(\"{} abbreviations detected and kept ({} omitted)\".format(written, omit))\n",
    "\n",
    "    # Return most common definition for each term\n",
    "    if collect_definitions:\n",
    "        if most_common_definition:\n",
    "            # Return the most common definition for each term\n",
    "            for k,v in list_abbrev_map.items():\n",
    "                counter_abbrev_map[k] = Counter(v).most_common(1)[0][0]\n",
    "        elif first_definition:\n",
    "            # Return the first definition for each term\n",
    "            for k, v in list_abbrev_map.items():\n",
    "                counter_abbrev_map[k] = v\n",
    "        elif all_definition:\n",
    "            for k, v in list_abbrev_map.items():\n",
    "                counter_abbrev_map[k] = v\n",
    "        return counter_abbrev_map\n",
    "\n",
    "    # Or return the last encountered definition for each term\n",
    "    return abbrev_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(base_dir,file_list):\n",
    "    files = os.listdir(base_dir)\n",
    "    for i in files:\n",
    "        abs_path = os.path.join(base_dir,i)\n",
    "        if re.match(r'(.*)PMC(.*).html',abs_path) and ('highlighted' not in abs_path):\n",
    "            file_list.append(abs_path)\n",
    "        elif os.path.isdir(abs_path)&('ipynb_checkpoints' not in abs_path):\n",
    "            get_files(abs_path,file_list)\n",
    "    return file_list\n",
    "            \n",
    "def remove_duplicates_files(file_list):\n",
    "    file_list_without_duplicates=[]\n",
    "    for file in file_list:\n",
    "        PMC_number=file.split('/')[-1]\n",
    "        duplicates_status=False\n",
    "        for f in file_list_without_duplicates:\n",
    "            if PMC_number in f:\n",
    "                duplicates_status=True\n",
    "        if not duplicates_status:\n",
    "            file_list_without_duplicates.append(file)\n",
    "    return file_list_without_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find/write html with a list of abbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_write_html_with_abbre(soup_og):\n",
    "    heading = soup_og.find_all('h2')\n",
    "    abbre_exist=False\n",
    "    for number, element in enumerate(heading):\n",
    "        header = element.get_text()\n",
    "        if re.search('abbreviation',header,re.IGNORECASE):\n",
    "            abbre_exist=True\n",
    "            directory=''\n",
    "            for folder in file.split('/')[:-1]:\n",
    "                directory +=folder+'/'\n",
    "            path = directory.replace('data','outputs/abbre')\n",
    "            try:os.makedirs(path)\n",
    "            except:continue\n",
    "            break\n",
    "            \n",
    "    if abbre_exist:\n",
    "        with open(file.replace('data','outputs/abbre'),\"w\",encoding='UTF-8') as f:\n",
    "            f.write(str(soup_og))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for file in file_list:\n",
    "    with open(file,'r',encoding='UTF-8',errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        find_write_html_with_abbre(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlight Abbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def highlight_SF_LF(pairs):\n",
    "    soup_new=str(soup)\n",
    "    for key,value in pairs.items():\n",
    "        #print(key+':'+value)\n",
    "        soup_new=soup_new.replace(str(key),'<span style=\"background:yellow;\">'+str(key)+'</span>') # Color All SF\n",
    "        #soup_new=soup_new.replace(str(value),'<span style=\"background:yellow;\">'+str(value)+'</span>') # Color All LF\n",
    "    return soup_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract abbre （table/list/plain text）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abbreviation(file_list):\n",
    "    text_to_be_written=''\n",
    "    whole_dict={}\n",
    "    for file in file_list:\n",
    "        with open(file,'r',encoding='UTF-8',errors='ignore') as f:\n",
    "            text = f.read()\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            main_text=extract_particular_paragraph(soup,file)\n",
    "            pairs = extract_abbreviation_definition_pairs(doc_text=main_text,most_common_definition=True)\n",
    "            \n",
    "            # highlight in html\n",
    "            #with open(file.replace('.html','_highlighted.html'),'w') as f3:\n",
    "            #    soup_SH=highlight_SF_LF(pairs)\n",
    "            #    f3.write(soup_SH)\n",
    "            #    whole_dict=merge_pair_Dict(pairs,whole_dict)\n",
    "            \n",
    "            # write dict extracted from main text\n",
    "            with open('/rdsgpfs/general/user/yh4218/home/outputs/abbreviation/'+file.split('/')[-1].split('.')[0]+'_from_main_text.json','w', encoding='utf8') as f5:\n",
    "                json.dump(pairs,f5,indent=2,ensure_ascii=False)\n",
    "            #    print (file.split('/')[-1]+'  :  '+str(pairs))\n",
    "            \n",
    "            # write dict given by author\n",
    "            abbre_dict=get_abbre_dict_given_by_author(soup)\n",
    "            if abbre_dict:\n",
    "                with open('/rdsgpfs/general/user/yh4218/home/outputs/abbreviation/'+file.split('/')[-1].split('.')[0]+'_from_author.json','w', encoding='utf8') as f6:\n",
    "                    json.dump(abbre_dict,f6,indent=2,ensure_ascii=False)\n",
    "            \n",
    "            # write all result into one file\n",
    "            #text_to_be_written+=file.split('/')[-1]+'\\n'\n",
    "            #for SF, LF in pairs.items():\n",
    "            #    text_to_be_written += SF+'\\t'+LF+'\\n'\n",
    "\n",
    "    #with open('/rdsgpfs/general/user/yh4218/home/outputs/abbre/list_of_abbre.txt','w') as f4:\n",
    "    #    f4.write(text_to_be_written)\n",
    "    \n",
    "    return\n",
    "\n",
    "def merge_pair_Dict(pairs,whole_pair_dict):\n",
    "    for SF, LF in pairs.items():\n",
    "        if SF in list(whole_pair_dict.keys()):\n",
    "            if LF.lower() not in whole_pair_dict[SF]:\n",
    "                whole_pair_dict[SF].append(LF.lower())\n",
    "        else:\n",
    "            whole_pair_dict[SF]=[LF.lower()]\n",
    "    return whole_pair_dict\n",
    "\n",
    "def listToDict(lst):\n",
    "    op = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return op\n",
    "\n",
    "def abbre_table_to_dict(t):\n",
    "    abbre_list=[]\n",
    "    rows = t.findAll(\"tr\")\n",
    "    for i in rows:\n",
    "        elements = i.findAll(['td', 'th'])\n",
    "        vals = [j.get_text() for j in elements]\n",
    "        abbre_list+=vals\n",
    "    abbre_dict=listToDict(abbre_list)\n",
    "    return abbre_dict\n",
    "\n",
    "def abbre_list_to_dict(t):\n",
    "    abbre_list=[]\n",
    "    SF = t.findAll(\"dt\")\n",
    "    SF_list = [SF_word.get_text() for SF_word in SF]\n",
    "    LF = t.findAll(\"dd\")\n",
    "    LF_list = [LF_word.get_text() for LF_word in LF]\n",
    "    abbre_dict=dict(zip(SF_list, LF_list))\n",
    "    return abbre_dict\n",
    "    \n",
    "def get_abbre_plain_text(soup_og):\n",
    "    abbre_text=soup_og.get_text()\n",
    "    abbre_list=abbre_text.split(';')\n",
    "    list_lenth=len(abbre_list)\n",
    "    return abbre_list,list_lenth\n",
    "\n",
    "def get_abbre_dict_given_by_author(soup_og):\n",
    "    header = soup_og.find_all('h2',recursive=True)\n",
    "    abbre_dict={}\n",
    "    for number, element in enumerate(header):\n",
    "        if re.search('abbreviation',element.get_text(),re.IGNORECASE):\n",
    "            nearest_down_tag = element.next_element\n",
    "            while nearest_down_tag:\n",
    "                tag_name = nearest_down_tag.name\n",
    "                \n",
    "            # when abbre is table\n",
    "                if tag_name == 'table':\n",
    "                    abbre_dict=abbre_table_to_dict(nearest_down_tag)\n",
    "                    break\n",
    "\n",
    "            # when abbre is list\n",
    "                elif tag_name=='dl':\n",
    "                    abbre_dict=abbre_list_to_dict(nearest_down_tag)\n",
    "                    break\n",
    "\n",
    "            # when abbre is plain text\n",
    "                elif tag_name=='p':\n",
    "                    abbre_list,list_lenth = get_abbre_plain_text(nearest_down_tag)\n",
    "                    if list_lenth<=2:\n",
    "                        nearest_down_tag = nearest_down_tag.next_element\n",
    "                        continue\n",
    "                    else:\n",
    "                        for abbre_pair in abbre_list:\n",
    "                            if len(abbre_pair.split(':'))==2:abbre_dict.update({abbre_pair.split(':')[0]:abbre_pair.split(':')[1]})\n",
    "                            elif len(abbre_pair.split(','))==2:abbre_dict.update({abbre_pair.split(',')[0]:abbre_pair.split(',')[1]})\n",
    "                            elif len(abbre_pair.split(' '))==2:abbre_dict.update({abbre_pair.split(' ')[0]:abbre_pair.split(' ')[1]})\n",
    "                        break\n",
    "                        \n",
    "            # search until next h2\n",
    "                elif tag_name=='h2':\n",
    "                    break\n",
    "                else:\n",
    "                    nearest_down_tag = nearest_down_tag.next_element\n",
    "    return abbre_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Write main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_list=['funding',\n",
    "              'Conflicts of Interest',\n",
    "              #'Additional Information',\n",
    "              'Acknowledgment',\n",
    "              'Acknowledgement',\n",
    "              'financial',\n",
    "              'contribution',\n",
    "              'conflict',\n",
    "              'footnotes',\n",
    "              'interest',\n",
    "              'Reference',\n",
    "              'Formats',\n",
    "              'Share',\n",
    "              'Patent',\n",
    "              'disclosure',\n",
    "              'Availability',\n",
    "              'additional']\n",
    "abstract_list=['abstract']\n",
    "introduction_list=['introduction']\n",
    "background_list=['background']\n",
    "overview_list=['overview']\n",
    "supplementary_material_list=['supplementary','Associated Data','Supplemental']\n",
    "abbreviation_list=['glossary','abbreviation']\n",
    "method_material_list=['method','material','Experimental'] # sometimes titled Experimental section & procedure \n",
    "result_discussion_list=['Results','discussion']\n",
    "conclusion_list=['conclusion']  # sometimes with future direction\n",
    "keep_list=abstract_list+introduction_list+background_list+overview_list+supplementary_material_list+method_material_list+result_discussion_list+conclusion_list\n",
    "\n",
    "def extract_particular_paragraph(soup_og,filepath):\n",
    "    main_text=''\n",
    "    paragraph = soup_og.find_all('p',attrs={'id': re.compile('(__|_|)(p|P|Par|par|idm|para)\\d+')})\n",
    "    for number, element in enumerate(paragraph):\n",
    "        nearest_tag = element.previous_element\n",
    "        check = False\n",
    "        while nearest_tag:\n",
    "            if nearest_tag.name != 'h2':\n",
    "                nearest_tag = nearest_tag.previous_element\n",
    "            else:\n",
    "                for keep_word in keep_list:\n",
    "                    if re.search(keep_word,nearest_tag.get_text(),re.IGNORECASE):\n",
    "                        for discard_word in discard_list:\n",
    "                            if not re.search(discard_word,nearest_tag.get_text(),re.IGNORECASE):\n",
    "                                check = True                                \n",
    "                break\n",
    "                \n",
    "        if check:\n",
    "            #element.attrs['style'] =\"color:red;\"\n",
    "            main_text+=element.get_text().replace('\\n',' ')\n",
    "\n",
    "    return main_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract abbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input base directory: /rdsgpfs/general/user/yh4218/home/outputs/abbre/NLP/NLP-MWAS\n"
     ]
    }
   ],
   "source": [
    "# files with abbre list\n",
    "# MWAS\n",
    "#base_dir = '/rdsgpfs/general/user/yh4218/home/outputs/abbre/NLP/NLP-MWAS'\n",
    "# GWAS\n",
    "#base_dir = '/rdsgpfs/general/user/yh4218/home/outputs/abbre/NLP/NLP-GWAS'\n",
    "\n",
    "if __name__=='__main__':\n",
    "    base_dir = input('Input base directory:')\n",
    "    file_list = []\n",
    "    file_list = get_files(base_dir,file_list)\n",
    "    file_list = remove_duplicates_files(file_list)\n",
    "    file_list=['/rdsgpfs/general/user/yh4218/home/data/civil eng/Effect of grout properties on shear strength of column base connections_ FEA and analytical approach - ScienceDirect.html']\n",
    "    extract_abbreviation(file_list[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ab3P method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('cd /rdsgpfs/general/user/yh4218/home/Ab3P; /rdsgpfs/general/user/yh4218/home/Ab3P/identify_abbr /rdsgpfs/general/user/yh4218/home/data/main_text.txt > /rdsgpfs/general/user/yh4218/home/data/test.txt',shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ab3P results to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IDIBAPS': 'Institut d’Investigacions Biomediques August Pi Sunyer', 'NAFLD': 'nonalcoholic fatty liver disease', 'UPLC-MS': 'ultra-performance liquid chromatography coupled to mass spectrometry', 'BMI': 'body-mass index', 'TAG': 'triacylglycerides', 'TOF': 'UPLC-time-of-flight', 'AC': 'acyl carnitines', 'DAG': 'diacylglycerides', 'ChoE': 'cholesterol esters', 'SM': 'sphingomyelin', 'Cer': 'ceramides', 'IS': 'internal standard', 'RSD': 'relative standard deviation', 'ROC': 'Receiver operating characteristic', 'PUFA': 'polyunsaturated fatty acid', 'VLDL': 'very low density lipoprotein', 'AUC': 'accuracy'}\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/rdsgpfs/general/user/yh4218/home/data/test.txt\", \"r\")\n",
    "keys=[]\n",
    "values=[]\n",
    "for line in f.readlines()[1:]:\n",
    "    if float(line.split('|')[2].strip())>0.97:\n",
    "        #print (float(line.split('|')[2].strip()))\n",
    "        keys.append(line.split('|')[0].strip())\n",
    "        values.append(line.split('|')[1].strip())\n",
    "abbre_dict=dict(zip(keys,values))\n",
    "f.close()\n",
    "print (abbre_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
